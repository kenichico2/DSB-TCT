{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT日本語 （encode_plusバージョン）",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPXGb8dDXM4Zbg7Ttb8358S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/macken1/DSB-TCT/blob/master/BERT%E6%97%A5%E6%9C%AC%E8%AA%9E_%EF%BC%88encode_plus%E3%83%90%E3%83%BC%E3%82%B8%E3%83%A7%E3%83%B3%EF%BC%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jLSiVoAmcln",
        "colab_type": "text"
      },
      "source": [
        "# **BERTを使って文をベクトルで表現してみる**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kBMtIemhcCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 以下、Mecabを入れとかないとTokenizerでエラーとなるのでインストールしておく\n",
        "## 乾研のBERTではmecab-ipadic-2.7.0-20070801との要件になっているが、最新版をインストール。影響は不明\n",
        "!apt install aptitude\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw2Q3gVqmrw2",
        "colab_type": "text"
      },
      "source": [
        "## 1. 形態素解析（単語に分割する）ツールのMeCabをインストールする\n",
        "\n",
        "Google colaboratory環境に、MeCab関連のツール（MeCab単体＋辞書、MeCabをpythonから実行）などをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JknZqHhorilg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "# 念のため、形態素解析ができるかチェック\n",
        "\n",
        "import MeCab\n",
        "\n",
        "m = MeCab.Tagger(\"Owakati\")\n",
        "print(m.parse(\"私はKARAが大好きだった。コンサートに行ったころが懐かしい。\"))\n",
        "print(m.parse(\"コロナウイルスとプロスペクト理論に悩む今日この頃\"))\n",
        "print(m.parse(\"すもももももももものうち\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49YQ_SX1m2Ek",
        "colab_type": "text"
      },
      "source": [
        "Colabでグラフ表示する際に文字化けしないように、必要なライブラリをインストールします"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGZI6iaMpqDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#　以下はmatplotlibでの日本語表示用のライブラリ\n",
        "!pip install japanize_matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib \n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(font=\"IPAexGothic\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ot0imlwjApw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
        "import transformers as ppb\n",
        "\n",
        "# 東北大学乾研が作成した学習済みの日本語BERTモデルの形態素解析モデルを搭載します。\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained('bert-base-japanese-whole-word-masking')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O3eAgEPnV5-",
        "colab_type": "text"
      },
      "source": [
        "参照用にBERTモデル内の語彙（形態素）をvocabulary.txtに書き出します。<br>左のウインドウからvocabulary.txtをダブルクリックすると内容が確認できます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRf7NXJGgsnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write all tokens in vocabulary.txt, which you can find on the left-hand side\n",
        "with open(\"vocabulary.txt\", 'w') as f:\n",
        "    \n",
        "    # For each token...\n",
        "    for token in tokenizer.vocab.keys():\n",
        "        \n",
        "        # Write it out and escape any unicode characters.            \n",
        "        f.write(token + '\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seprombPowgF",
        "colab_type": "text"
      },
      "source": [
        "辞書に含まれている形態素の長さ（＝文字カウント）について分布をみてみます"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjvrWcNTdkAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "sns.set(style='darkgrid',font=\"IPAexGothic\")\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5,font=\"IPAexGothic\")\n",
        "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
        "\n",
        "# Measure the length of every token in the vocab.\n",
        "token_lengths = [len(token) for token in tokenizer.vocab.keys()]\n",
        "\n",
        "# Plot the number of tokens of each length.\n",
        "sns.countplot(token_lengths)\n",
        "plt.title('形態素の長さの分布')\n",
        "plt.xlabel('形態素の長さ')\n",
        "plt.ylabel('形態素数')\n",
        "\n",
        "print('最大の形態素長:', max(token_lengths))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np2tMLGvtOZ-",
        "colab_type": "text"
      },
      "source": [
        "文のベクトルを使って似た文を抽出してみましょう\n",
        "せっかくなので、たくさんの文章からベクトル化を使って意味の似た文章を抽出してみましょう。  \n",
        "まず、データを用意します。  \n",
        "京都大学の黒橋・河原研究室のホームページ(※1)から「Textual Entailment 評価データ」をダウンロードしてデータファイルを作ります。\n",
        "※１　https://bit.ly/2sXN2er"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2C-Asz9hLnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://nlp.ist.i.kyoto-u.ac.jp/nl-resource/rte/entail_evaluation_set.xml -P /content/\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "import codecs\n",
        "\n",
        "# 出力するファイルを指定\n",
        "f1 = codecs.open('T_plain.txt', 'w', 'utf-8')\n",
        "\n",
        "# XMLファイルからT1とT2の正例を抽出して、ファイルに出力\n",
        "corpus = ET.parse('entail_evaluation_set.xml')\n",
        "root = corpus.getroot()\n",
        "\n",
        "for child in root:\n",
        "  grandchild_text = {}\n",
        "  entail_tag = child.get('label')\n",
        "  for grandchild in child.getchildren():\n",
        "    grandchild_text[grandchild.tag] = grandchild.text\n",
        "  if entail_tag == '◎' or entail_tag == '〇':\n",
        "    f1.write(grandchild_text['t1']+\"\\n\")\n",
        "    f1.write(grandchild_text['t2']+\"\\n\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpg1DuNIpZFi",
        "colab_type": "text"
      },
      "source": [
        "どんな文が含まれているのか、10個ほどサンプリングして表示してみます"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5MOmBW0_IH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_table(\"/content/T_plain.txt\", header=None)\n",
        "df.sample(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtvu7sGXrp-9",
        "colab_type": "text"
      },
      "source": [
        "文例をいったんリスト化します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOHl21NG7PqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df[0].values\n",
        "print(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpG4MTawpirN",
        "colab_type": "text"
      },
      "source": [
        "実際に、日本語の文章がどのように形態素、さらにBERTの分析用のIDに変換されるのかを見てみてましょう"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECWG30I-7kAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the original sentence.\n",
        "print('原文: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('形態素: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('形態素に対応するID番号化: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIoQC5Txpthk",
        "colab_type": "text"
      },
      "source": [
        "文をBERTで分析する際、入力側の形態素数（≒単語数）を固定する必要があります。\n",
        "\n",
        "「Textual Entailment 評価データ」の形態素の数を見てみましょう。\n",
        "\n",
        "※CLS、SEPなどの特別な形態素も含めてカウントしています"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZhaoL1T71fF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('最長の形態素数は⇒ ', max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjw9FwX9rTYi",
        "colab_type": "text"
      },
      "source": [
        "BERTモデルを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rrkg0dNrDrgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertJapaneseTokenizer, BertModel\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-japanese-whole-word-masking')\n",
        "\n",
        "# Set the model in evaluation mode to deactivate the DropOut modules\n",
        "# This is IMPORTANT to have reproducible results during evaluation!\n",
        "# どうもpytorchには学習モードと推論モードがあるらしい。以下は推論モードに切り替え\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3CNYOOhrhq0",
        "colab_type": "text"
      },
      "source": [
        "文例を一気に形態素の順にID化します。ここでは同時に短い文例は32形態素になるよう０で残りを埋めています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM0JXUrn8E9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 32,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "print('attention_masks:', attention_masks[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoF0T-t4szbX",
        "colab_type": "text"
      },
      "source": [
        "ID化した文例をBERTモデルを使って形態素ごとにベクトル化し、アウトプットのテンソルの次元を確認します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxGxAw89MtVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GPU上で走らせるためには、関連するものをすべてGPU上に配置\n",
        "input_ids = input_ids.to('cuda')\n",
        "attention_masks = attention_masks.to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "# いよいよ推論させる、というか各形態素に対応するベクトルを計算\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask=attention_masks)\n",
        "# 以下で素のアウトプット\n",
        "last_hidden_states = outputs[0]\n",
        "last_hidden_states.shape\n",
        "# last_hidden_states.type"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBS06vsLtpAt",
        "colab_type": "text"
      },
      "source": [
        "吐き出したテンソルのうち、CLSに相当するテンソルのみ切り出します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYgnjnGVM_7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NumPy配列ndarrayの要素や部分配列（行・列など）は[2, 3, 1, ...]のように各次元の位置や範囲をカンマ区切りで指定。[:,0,:]でCLSに対応するはず\n",
        "last_hidden_states[:,0,:].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT0h8Rost2jN",
        "colab_type": "text"
      },
      "source": [
        "例に似た文章をコサイン類似度を使って似ている順番に抽出します。まず、形態素×ID化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23qPqDdmAsXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 例文と類似する文を探してみる\n",
        "example = '猫がニャーニャーとうるさくて眠れない。'\n",
        "\n",
        "# encode_plusでpaddingまでできる。超便利。ptでPyTorch、tfでTensorflow、何もつけないと普通にリストを返す\n",
        "example_ids = tokenizer.encode_plus(example, max_length=32, pad_to_max_length=True, return_tensors='pt')\n",
        "print(example_ids)\n",
        "type(example_ids)\n",
        "print(example_ids[\"input_ids\"]) #辞書から必要な部分を参照"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfQPDIJHuH9T",
        "colab_type": "text"
      },
      "source": [
        "そのうえで、BERTを使ってベクトル化します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO_cA1cMI85Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GPU上で走らせるためには、関連するものをすべてGPU上に配置\n",
        "input_ids_example = example_ids[\"input_ids\"].to('cuda')\n",
        "attention_masks_example = example_ids[\"attention_mask\"].to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "# いよいよ推論させる、というか各形態素に対応するベクトルを計算\n",
        "with torch.no_grad():\n",
        "    example_outputs = model(input_ids_example, attention_mask=attention_masks_example)\n",
        "# 以下で素のアウトプット\n",
        "last_hidden_states_example = example_outputs[0]\n",
        "last_hidden_states_example[:,0,:].shape\n",
        "# last_hidden_states.type"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTErxLktuRRE",
        "colab_type": "text"
      },
      "source": [
        "コサイン類似度をすべて文例と計算し、上位20例を表示します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWezvQ4Ad2Kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = codecs.open('/content/T_plain.txt', 'r', 'utf-8')\n",
        "corpussimdic = {}\n",
        "\n",
        "# コサイン類似度を計算する\n",
        "def cos_sim(v1, v2):\n",
        "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "# 例文のベクトル\n",
        "example_vec = last_hidden_states_example[:,0,:].to('cpu').detach().numpy().copy()\n",
        "\n",
        "# コーパス文のベクトル\n",
        "corpus_vec = last_hidden_states[:,0,:].to('cpu').detach().numpy().copy() \n",
        "  \n",
        "print()\n",
        "print(example, \"<=>\")\n",
        "\n",
        "#\n",
        "# コーパスの文とのコサイン類似度を求める\n",
        "#\n",
        "i = 0\n",
        "for sentence in corpus:\n",
        "\n",
        "  corpussimdic[sentence.rstrip('\\n')] = cos_sim(example_vec, corpus_vec[i,:])\n",
        "  i += 1\n",
        "\n",
        "\n",
        "# valueで降順にソートしてトップ20の類似文を表示する\n",
        "count = 0\n",
        "for k, v in sorted(corpussimdic.items(), key=lambda x: -x[1]):\n",
        "    print(str(v) + \": \" + str(k)) \n",
        "    count += 1 \n",
        "    if count == 20:\n",
        "      break\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}